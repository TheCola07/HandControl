{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp      # 使用mediapipe进行手部姿态估计\n",
    "import numpy as np\n",
    "import cv2\n",
    "from cvzone.HandTrackingModule import HandDetector  # 开源计算机视觉包，可进行人脸检测、姿态估计等应用，这里使用手部跟踪\n",
    "import autopy   # 获得鼠标的控制\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21个手部关键点信息\n",
    "\n",
    "cvzone.HandTrackingModule.HandDetector()手部关键点检测方法\n",
    "参数：\n",
    "\n",
    "mode：默认False，将输入图像视为视频流。它将尝试在第一个输入图像中检测手，并在成功检测后进一步定位手的坐标。在随后的图像中，一旦检测到所有 maxHands 手并定位了相应的手的坐标，它就会跟踪这些坐标，而不会调用另一个检测，直到它失去对任何一只手的跟踪。这减少了延迟，非常适合处理视频帧。如果设置为 True，则在每个输入图像上运行手部检测，用于处理一批静态的、可能不相关的图像。\n",
    "\n",
    "maxHands:最多检测几只手、默认为2\n",
    "\n",
    "detectionCon：手部检测模型的最小置信值（0-1之间），超过阈值则检测成功。默认为 0.5\n",
    "\n",
    "minTrackingCon： 坐标跟踪模型的最小置信值 (0-1之间)，用于将手部坐标视为成功跟踪，不成功则在下一个输入图像上自动调用手部检测。将其设置为更高的值可以提高解决方案的稳健性，但代价是更高的延迟。如果 mode 为 True，则忽略这个参数，手部检测将在每个图像上运行。默认为 0.5\n",
    "\n",
    "它的参数和返回值类似于官方函数 mediapipe.solutions.hands.Hands()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cvzone.HandTrackingModule.HandDetector.findHands()找到手部关键点并绘图\n",
    "参数\n",
    "\n",
    "img： 需要检测关键点的帧图像，格式为BGR\n",
    "\n",
    "draw： 是否需要在原图像上绘制关键点及识别框\n",
    "\n",
    "flipType： 图像是否需要翻转，当视频图像和我们自己不是镜像关系时，设为True就可以了\n",
    "\n",
    "返回值：\n",
    "\n",
    "hands： 检测到的手部信息，包含：21个关键点坐标，检测框坐标及宽高，检测框中心坐标，检测出是哪一只手\n",
    "\n",
    "img： 返回绘制了关键点及连线后的图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入视频数据\n",
    "cap = cv2.VideoCapture(0)   # 0代表自己电脑的摄像头\n",
    "cap.set(3, 1280)  # 设置显示框的宽度\n",
    "cap.set(4, 720)   # 设置显示框的高度\n",
    "# cap.set(3, cap.get(cv2.CAP_PROP_FRAME_WIDTH)) # 获取视频的宽度\n",
    "# cap.set(4, cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "pTime = 0   #设置第一帧开始处理的时间\n",
    "\n",
    "#进行手部检测\n",
    "detector = HandDetector(staticMode=False,     # 视频流图像\n",
    "                        maxHands=1,     # 最多检测一只手\n",
    "                        detectionCon=0.8,# 最小置信度\n",
    "                        minTrackCon=0.9) # 最小跟踪置信度\n",
    "\n",
    "# 主循环过程\n",
    "while True:\n",
    "    \n",
    "    # 图片是否成功接收、img帧图像\n",
    "    success, img = cap.read()\n",
    "    \n",
    "    # 翻转图像，使自身和摄像头中的自己呈现镜像关系\n",
    "    img = cv2.flip(img, flipCode=1) # 1代表水平翻转，0代表竖直翻转\n",
    "    \n",
    "    # 手部检测\n",
    "    # 传入每帧图像，返回手部关键点的坐标信息（字典构成的列表hands），绘制关键点后的图像img\n",
    "    hands, img = detector.findHands(img, flipType=False)    # 上面翻转过了，无需再翻转\n",
    "    # print(hands)\n",
    "    \n",
    "    # 显示图像\n",
    "    # 加入FPS信息\n",
    "    cTime = time.time() # 处理完一帧图像的时间\n",
    "    fps = 1/(cTime - pTime)\n",
    "    pTime = cTime\n",
    "    \n",
    "    # 在视频上显示fps信息，先转换成整数再变成字符串形式，文本显示坐标，文本字体，文本大小\n",
    "    cv2.putText(img, str(int(fps)), (70, 50), cv2.FONT_HERSHEY_PLAIN, 3, (255, 0, 0), 3)\n",
    "    \n",
    "    # 显示图像，输入窗口名及图像数据\n",
    "    cv2.imshow('image', img)\n",
    "    if cv2.waitKey(1) & 0xFF==27:       # 每帧滞留20毫秒后消失，ESC键退出\n",
    "        break\n",
    "    \n",
    "# 释放视频资源\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "移动鼠标\n",
    "移动鼠标的思路是：如果检测到食指竖起，并且中指弯下，那么就认为是移动鼠标，鼠标的位置坐标是食指指尖所在的位置坐标。\n",
    "\n",
    "检测哪个手指是竖起的方法是 detector.fingersUp() ，传入检测到的某只手的手部信息hands[0]；返回值是由5个元素构成的列表，元素为1代表该手指竖起，0代表手指弯下，例如：[0,1,1,0,0] 就代表食指和中指竖起，其他手指弯下。\n",
    "\n",
    "当手指在摄像头画面的下半部分移动时，由于摄像头界限范围问题，手掌部分会在摄像头画面中消失，导致检测不到手部关键点，因此，在屏幕画面的偏上半部分绘制一个黄色的矩形框，手指只能在矩形框中移动，避免手部关键点的消失。\n",
    "\n",
    "由于我们设置的矩形框大小明显要小于电脑屏幕的大小，导致手控的鼠标无法在整个电脑屏幕上移动。因此，需要将矩形框的宽和高映射到电脑屏幕的宽和高。使用线性插值方法 np.interp(x, xp, fp)  简单来说就是将变量x的范围从原来的xp映射到fp。如：np.interp(x1, (pt1[0], pt2[0]), (0, wScr))，就是将x坐标的范围从原来的 pt1[0] 到 pt1[0]+w，映射到整个电脑屏幕 0 到 wScr。\n",
    "\n",
    "返回电脑屏幕的宽和高： autopy.screen.size()\n",
    "\n",
    "移动鼠标的位置到坐标(x,y)： autopy.mouse.move(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26800\\981772771.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m# 获取食指指尖的坐标，和中指指尖的坐标\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlmList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m      \u001b[1;31m# 食指尖的关键点为8\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mx2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlmList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m]\u001b[0m     \u001b[1;31m# 中指指尖的关键点为12\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 导入视频数据\n",
    "wScr, hScr = autopy.screen.size()   # 返回电脑屏幕的宽和高\n",
    "wCam, hCam = 1280, 720  # 视频显示窗口的宽和高\n",
    "pt1, pt2 = (100, 100), (1100, 500)  # 虚拟鼠标的移动范围，左上坐标pt1，右上坐标pt2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, wCam)\n",
    "cap.set(4, hCam)\n",
    "\n",
    "pTime = 0       # 设置第一帧开始处理的起始时间\n",
    "\n",
    "# 手部检测\n",
    "detector = HandDetector(staticMode=False,\n",
    "                        maxHands=1,\n",
    "                        detectionCon=0.8,\n",
    "                        minTrackCon=0.5)\n",
    "\n",
    "while True:\n",
    "    \n",
    "    success, img = cap.read()\n",
    "    \n",
    "    img = cv2.flip(img, flipCode=1)\n",
    "    \n",
    "    # 在图像窗口上创建一个矩形框，在该区域内移动鼠标\n",
    "    cv2.rectangle(img, pt1, pt2, (0, 255, 255), 5)\n",
    "    \n",
    "    hands, img = detector.findHands(img, flipType=False)\n",
    "    \n",
    "    if hands:\n",
    "        \n",
    "        # 获取手部信息hands中的21个关键点信息\n",
    "        lmList = hands[0]['lmList']     # 获取的手部信息由N个字典组成\n",
    "        \n",
    "        # 获取食指指尖的坐标，和中指指尖的坐标\n",
    "        x1, y1 = lmList[8]      # 食指尖的关键点为8\n",
    "        x2, y2 = lmList[12]     # 中指指尖的关键点为12\n",
    "        \n",
    "        # 检测哪个手指是朝上的\n",
    "        fingers = detector.fingersUp(hands[0])\n",
    "        # print(fingers)\n",
    "        \n",
    "        if fingers[1] == 1 and fingers[2] == 0:\n",
    "            # 开始移动时，在食指指尖画一个圆圈来表示\n",
    "            cv2.circle(img, (x1, y1), 15, (255, 255, 0), cv2.FILLED)    # 圆为填充圆\n",
    "            \n",
    "            # 确定鼠标移动的范围\n",
    "            # 将食指的移动范围从预制的窗口范围，映射到电脑屏幕范围\n",
    "            x3 = np.interp(x1, (pt1[0], pt2[0]), (0, wScr))\n",
    "            y3 = np.interp(y1, (pt1[1], pt2[1]), (0, hScr))\n",
    "            \n",
    "            # 鼠标移动\n",
    "            autopy.mouse.move(x3, y3)   # 给出鼠标移动位置坐标\n",
    "            \n",
    "    # 显示图像\n",
    "    # 查看FPS\n",
    "    cTime = time.time()\n",
    "    fps = 1/(cTime - pTime)\n",
    "    pTime = cTime\n",
    "    \n",
    "    cv2.putText(img, str(int(fps)), (70, 50), cv2.FONT_HERSHEY_PLAIN, 3, (255, 0, 0), 3)\n",
    "    \n",
    "    cv2.imshow('img', img)\n",
    "    if cv2.waitKey(1) & 0xFF==27:\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "点击鼠标的思路是：如果食指和中指同时竖起，并且食指指尖和中指指尖之间的像素距离小于50时，那么就认为是点击鼠标。\n",
    "\n",
    "检测哪个手指是竖起的方法是上面已经解释过的 detector.fingersUp() 方法，检测指尖距离的方法是： detector.findDistance(pt1, pt2, img)， pt1 和 pt2 是两个点的坐标，传入img来绘制指尖连线图。\n",
    "\n",
    "点击鼠标的函数，autopy.mouse.click()\n",
    "\n",
    "由于用手指控制鼠标时，每一帧的坐标位置的变化幅度较大，导致电脑鼠标在手指控制下很容易晃动，很难准确定位到一个目标。因此需要平滑每一帧的坐标变化，使坐标变化更缓慢一些。\n",
    "\n",
    "例如：cLocx = pLocx + (x3 - pLocx) / smooth，式中：前帧的鼠标位置的x坐标 cLocx；前一帧的鼠标位置的x坐标 pLocx；当前鼠标位置的x坐标 x3；自定义平滑系数smooth，值越大鼠标移动就越慢，平稳性就越高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from cvzone.HandTrackingModule import HandDetector   # 手部检测方法\n",
    "import time\n",
    "import autopy\n",
    " \n",
    "#（1）导数视频数据\n",
    "wScr, hScr = autopy.screen.size()   # 返回电脑屏幕的宽和高(1920.0, 1080.0)\n",
    "wCam, hCam = 1280, 720   # 视频显示窗口的宽和高\n",
    "pt1, pt2 = (100,100), (1100, 500)   # 虚拟鼠标的移动范围，左上坐标pt1，右下坐标pt2\n",
    " \n",
    "cap = cv2.VideoCapture(0)  # 0代表自己电脑的摄像头\n",
    "cap.set(3, wCam)  # 设置显示框的宽度1280\n",
    "cap.set(4, hCam)  # 设置显示框的高度720\n",
    " \n",
    "pTime = 0  # 设置第一帧开始处理的起始时间\n",
    " \n",
    "pLocx, pLocy = 0, 0  # 上一帧时的鼠标所在位置\n",
    " \n",
    "smooth = 4  # 自定义平滑系数，让鼠标移动平缓一些\n",
    " \n",
    "#（2）接收手部检测方法\n",
    "detector = HandDetector(mode=False,  # 视频流图像 \n",
    "                        maxHands=1,  # 最多检测一只手\n",
    "                        detectionCon=0.8,  # 最小检测置信度 \n",
    "                        minTrackCon=0.5)   # 最小跟踪置信度\n",
    " \n",
    "#（3）处理每一帧图像\n",
    "while True:\n",
    "    \n",
    "    # 图片是否成功接收、img帧图像\n",
    "    success, img = cap.read()\n",
    "    \n",
    "    # 翻转图像，使自身和摄像头中的自己呈镜像关系\n",
    "    img = cv2.flip(img, flipCode=1)  # 1代表水平翻转，0代表竖直翻转\n",
    "    \n",
    "    # 在图像窗口上创建一个矩形框，在该区域内移动鼠标\n",
    "    cv2.rectangle(img, pt1, pt2, (0,255,255), 5)\n",
    "    \n",
    "    #（4）手部关键点检测\n",
    "    # 传入每帧图像, 返回手部关键点的坐标信息(字典)，绘制关键点后的图像\n",
    "    hands, img = detector.findHands(img, flipType=False)  # 上面反转过了，这里就不用再翻转了\n",
    "    # print(hands)\n",
    "    \n",
    "    # 如果能检测到手那么就进行下一步\n",
    "    if hands:\n",
    "        \n",
    "        # 获取手部信息hands中的21个关键点信息\n",
    "        lmList = hands[0]['lmList']  # hands是由N个字典组成的列表，字典包每只手的关键点信息\n",
    "        \n",
    "        # 获取食指指尖坐标，和中指指尖坐标\n",
    "        x1, y1 = lmList[8]  # 食指尖的关键点索引号为8\n",
    "        x2, y2 = lmList[12] # 中指指尖索引12\n",
    " \n",
    "        #（5）检查哪个手指是朝上的\n",
    "        fingers = detector.fingersUp(hands[0])  # 传入\n",
    "        # print(fingers) 返回 [0,1,1,0,0] 代表 只有食指和中指竖起\n",
    "        \n",
    "        # 如果食指竖起且中指弯下，就认为是移动鼠标\n",
    "        if fingers[1] == 1 and fingers[2] == 0:\n",
    "            \n",
    "            # 开始移动时，在食指指尖画一个圆圈，看得更清晰一些\n",
    "            cv2.circle(img, (x1,y1), 15, (255,255,0), cv2.FILLED)  # 颜色填充整个圆\n",
    " \n",
    "            #（6）确定鼠标移动的范围\n",
    "            # 将食指的移动范围从预制的窗口范围，映射到电脑屏幕范围\n",
    "            x3 = np.interp(x1, (pt1[0], pt2[0]), (0, wScr))\n",
    "            y3 = np.interp(y1, (pt1[1], pt2[1]), (0, hScr))\n",
    " \n",
    "            #（7）平滑，使手指在移动鼠标时，鼠标箭头不会一直晃动\n",
    "            cLocx = pLocx + (x3 - pLocx) / smooth  # 当前的鼠标所在位置坐标\n",
    "            cLocy = pLocy + (y3 - pLocy) / smooth            \n",
    "      \n",
    "            #（8）移动鼠标\n",
    "            autopy.mouse.move(cLocx, cLocy)  # 给出鼠标移动位置坐标\n",
    "            \n",
    "            # 更新前一帧的鼠标所在位置坐标，将当前帧鼠标所在位置，变成下一帧的鼠标前一帧所在位置\n",
    "            pLocx, pLocy = cLocx, cLocy\n",
    " \n",
    "        #（9）如果食指和中指都竖起，指尖距离小于某个值认为是点击鼠标\n",
    "        if fingers[1] == 1 and fingers[2] == 1:  # 食指和中指都竖起\n",
    "         \n",
    "            # 计算食指尖和中指尖之间的距离distance,绘制好了的图像img,指尖连线的信息info\n",
    "            distance, info, img = detector.findDistance((x1, y1), (x2, y2), img)\n",
    "            # print(distance)\n",
    "            \n",
    "            # 当指间距离小于50（像素距离）就认为是点击鼠标\n",
    "            if distance < 50:\n",
    "                \n",
    "                # 在食指尖画个绿色的圆，表示点击鼠标\n",
    "                cv2.circle(img, (x1,y1), 15, (0,255,0), cv2.FILLED)\n",
    "                \n",
    "                # 点击鼠标\n",
    "                autopy.mouse.click()\n",
    " \n",
    "    #（10）显示图像\n",
    "    # 查看FPS\n",
    "    cTime = time.time() #处理完一帧图像的时间\n",
    "    fps = 1/(cTime-pTime)\n",
    "    pTime = cTime  #重置起始时间\n",
    "    \n",
    "    # 在视频上显示fps信息，先转换成整数再变成字符串形式，文本显示坐标，文本字体，文本大小\n",
    "    cv2.putText(img, str(int(fps)), (70,50), cv2.FONT_HERSHEY_PLAIN, 3, (255,0,0), 3)  \n",
    "    \n",
    "    # 显示图像，输入窗口名及图像数据\n",
    "    cv2.imshow('image', img)    \n",
    "    if cv2.waitKey(1) & 0xFF==27:  #每帧滞留20毫秒后消失，ESC键退出\n",
    "        break\n",
    " \n",
    "# 释放视频资源\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
